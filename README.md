# HIFI: Explaining and Mitigating Algorithmic Bias Through the Lens of Game-Theoretic Interactions

This artifact contains the source code of HIFI and other existing state-of-the-art bias mitigation methods used in our paper, as well as the datasets and experiments. HIFI is an in-processing fairness improvement approach with game-theoretic analysis, and it is used as a regularizer during training. You can replicate the results reported in our paper by following this guideline.

## Experimental Environment

The following packages are required:
- `python==3.9.18`
- `numpy==1.23.5`
- `scipy`
- `cliffs_delta`
- `pandas==2.2.1`
- `joblib`
- `scikit-learn==1.4.2`
- `tensorflow==2.10.0`
- `matplotlib`
- `tqdm`
- `imblearn`
- `torch`
- `BlackBoxAuditing`
- `shapely`

We specify versions for some of them to prevent version conflicts. We directly include AIF360 ([AIF360 GitHub](https://github.com/Trusted-AI/AIF360)) in our artifact because the version we used is from [this repository](https://github.com/chenzhenpeng18/ICSE24-Multi-Attribute-Fairness/blob/main/aif360.zip), which was adapted to solve intersectional fairness issues.

Conda is recommended for all configurations. You can quickly install Miniconda ([Miniconda documentation](https://docs.anaconda.com/miniconda/)) if you have not installed Conda. We provide the environment files exported from our experimental environment, so you can get started quickly by running:

```sh
conda env create -f {Directory you saved our artifact}/XAI4Fairness/environments/environment.yml
```

If you want to build it from scratch, create a new environment with conda, and then simply run `conda {package}` or `pip {package}` to install the required packages.

## Scripts and Results

The artifact contains the following folders:

- `environments/` contains the experimental environments exported with conda and pip.
- `bias_mitigation_methods/` contains the implementation of our method HIFI and the combined methods (REW+HIFI, MAAT+HIFI, and FairMask+HIFI), as well as the default training process and 11 state-of-the-art bias mitigation methods.
- `data/` contains the original datasets, preprocessed datasets, and the implementation of data preprocessing.
- `harsanyi/` contains the code for computing Harsanyi interactions.
- `models/` contains the implementation of model standardization to output the probability of a positive label. Note that the models optimized by methods in `bias_mitigation_methods/` will be automatically saved here (we have deleted the trained models due to the large size).
- `analysis/` contains the code of our experiments.
- `tools/` contains the implementation of metric evaluation, configurations, and other tools.
- `plot_figs/` contains the code for plotting Fig.4-11.
- The raw data and intermediate results generated by programs in `analysis/` will be automatically saved to `results/` (we have deleted our intermediate results in this directory due to the large size).

## Replication

You can first train the subject models by running:

```sh
cd {Directory you saved our artifact}/
python bias_mitigation_methods/default.py
python bias_mitigation_methods/rew.py
python bias_mitigation_methods/dir.py
python bias_mitigation_methods/fairsmote.py
python bias_mitigation_methods/meta.py
python bias_mitigation_methods/adv.py
python bias_mitigation_methods/pr.py
python bias_mitigation_methods/eop.py
python bias_mitigation_methods/ceo.py
python bias_mitigation_methods/roc.py
python bias_mitigation_methods/maat.py
python bias_mitigation_methods/fairmask.py
python bias_mitigation_methods/hifi.py
python bias_mitigation_methods/rew+hifi.py
python bias_mitigation_methods/maat+hifi.py
python bias_mitigation_methods/fairmask+hifi.py
```

Afterwards, you can reproduce all the results as follows:

```sh
python analysis/interaction_analysis.py
python analysis/fairness_analysis.py
python analysis/time_cost_analysis.py
```

## Apply HIFI to Many More Tasks and Models

Our method HIFI can be applied to any model that can be trained using gradient-based optimization methods if sensitive attributes are explicitly specified, and we have implemented it with GPU acceleration, so you can replicate HIFI on more complex models with large-scale datasets. We add the fairness loss term in the `custom_loss` function of `hifi.py`, so you can revise the classification loss for your application and then use the loss computed by the `custom_loss` function to train your model.
